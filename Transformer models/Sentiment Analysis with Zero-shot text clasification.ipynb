{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cd9a4c",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Zero-shot text clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a918f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acfa86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create test cases\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"I absolutely love this!\",\n",
    "        \"It's fine, nothing special.\",\n",
    "        \"This is terrible, I'm disappointed.\",\n",
    "        \"Best decision ever.\",\n",
    "        \"Not bad, could be better.\",\n",
    "        \"I hate it here.\",\n",
    "        \"Meh, it's okay.\",\n",
    "        \"Fantastic work! Keep it up.\",\n",
    "        \"I'm not sure how I feel.\",\n",
    "        \"Worst experience ever.\"\n",
    "    ],\n",
    "    \"label\": ['positive', \"neutral\", \"negative\", 'positive', \"neutral\", \"negative\", \"neutral\", 'positive', \"neutral\", \"negative\"]  # Ground truth labels\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa634e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'I absolutely love this!', 'labels': ['positive', 'neutral', 'negative'], 'scores': [0.9621205925941467, 0.03216494619846344, 0.005714462138712406]}, {'sequence': \"It's fine, nothing special.\", 'labels': ['neutral', 'positive', 'negative'], 'scores': [0.9627881050109863, 0.029497094452381134, 0.007714795880019665]}, {'sequence': \"This is terrible, I'm disappointed.\", 'labels': ['negative', 'neutral', 'positive'], 'scores': [0.9974903464317322, 0.0015800370601937175, 0.0009296110947616398]}, {'sequence': 'Best decision ever.', 'labels': ['positive', 'neutral', 'negative'], 'scores': [0.9935014843940735, 0.0040102615021169186, 0.0024883218575268984]}, {'sequence': 'Not bad, could be better.', 'labels': ['neutral', 'positive', 'negative'], 'scores': [0.6571953892707825, 0.28543177247047424, 0.057372868061065674]}, {'sequence': 'I hate it here.', 'labels': ['negative', 'neutral', 'positive'], 'scores': [0.9935814142227173, 0.004364476073533297, 0.0020541155245155096]}, {'sequence': \"Meh, it's okay.\", 'labels': ['neutral', 'positive', 'negative'], 'scores': [0.9560512900352478, 0.03722498193383217, 0.006723757367581129]}, {'sequence': 'Fantastic work! Keep it up.', 'labels': ['positive', 'neutral', 'negative'], 'scores': [0.9930207133293152, 0.005442191381007433, 0.0015370820183306932]}, {'sequence': \"I'm not sure how I feel.\", 'labels': ['neutral', 'negative', 'positive'], 'scores': [0.581950843334198, 0.31934842467308044, 0.09870076924562454]}, {'sequence': 'Worst experience ever.', 'labels': ['negative', 'positive', 'neutral'], 'scores': [0.9975490570068359, 0.0013530448777601123, 0.0010979094076901674]}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"The product was okay, not too bad but not great either.\"\n",
    "labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "result = classifier(df[\"text\"].tolist(), labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a056f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'I absolutely love this!', 'label': 'positive'},\n",
       " {'sequence': \"It's fine, nothing special.\", 'label': 'neutral'},\n",
       " {'sequence': \"This is terrible, I'm disappointed.\", 'label': 'negative'},\n",
       " {'sequence': 'Best decision ever.', 'label': 'positive'},\n",
       " {'sequence': 'Not bad, could be better.', 'label': 'neutral'},\n",
       " {'sequence': 'I hate it here.', 'label': 'negative'},\n",
       " {'sequence': \"Meh, it's okay.\", 'label': 'neutral'},\n",
       " {'sequence': 'Fantastic work! Keep it up.', 'label': 'positive'},\n",
       " {'sequence': \"I'm not sure how I feel.\", 'label': 'neutral'},\n",
       " {'sequence': 'Worst experience ever.', 'label': 'negative'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top label for each dict in list\n",
    "result_list = [\n",
    "    {\n",
    "        'sequence': item['sequence'],\n",
    "        'label': item['labels'][item['scores'].index(max(item['scores']))]\n",
    "    }\n",
    "    for item in result\n",
    "]\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7dd8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pred_labels\"] = [x['label'] for x in result_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b9a7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00         3\n",
      "     neutral       1.00      1.00      1.00         4\n",
      "    positive       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ“‹ Classification Report:\\n\")\n",
    "print(classification_report(df[\"label\"], df[\"pred_labels\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
